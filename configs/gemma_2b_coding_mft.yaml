# configs/gemma_2b_coding_mft.yaml
# Mask Fine-Tuning configuration for Gemma-2 2B on coding domain
experiment_type: mft
domain: coding
base_model_path: "./outputs/gemma_2b_coding_fft/fft/coding/gemma_2b_coding_fft/final_fft_model"  # Actual FFT checkpoint path

model:
  model_name: "google/gemma-2-2b"
  max_seq_length: 512  # Match FFT config
  cache_dir: "./model_cache"

mask:
  mask_type: "local"
  sparsity_attn: 0.9  # Keep 90% of attention weights
  sparsity_mlp: 0.9   # Keep 90% of MLP weights
  masked_layers: [16, 17, 18, 19]  # Adjusted for Gemma's 26-layer architecture
  apply_to_attention: true
  apply_to_mlp: true
  subnet_mode: "both"

training:
  output_dir: "./outputs"
  experiment_name: "gemma_2b_coding_mft"
  num_train_epochs: 2
  learning_rate: 2.0e-5  # Same as FFT (paper uses same LR)
  warmup_ratio: 0.03
  lr_scheduler_type: "linear"
  weight_decay: 0.0
  logging_steps: 10
  save_steps: 10000  # Save only at end (5000 steps for 2 epochs, so 10000 > epochs)
  eval_steps: 10000  # Eval only at end
  seed: 42

data:
  dataset_mixer:
    "code_alpaca": 10000
    "evol_code": 10000
  max_train_samples: 20000
  max_eval_samples: 500
  preprocessing_num_workers: 4
  cache_dir: "./data_cache"
  dataloader_num_workers: 0
  dataloader_pin_memory: false

tpu:
  use_tpu: true
  batch_size_per_core: 1  # Match FFT to avoid OOM
  num_tpu_cores: 8
  gradient_accumulation_steps: 4  # Match FFT
  mixed_precision: "bf16"
  gradient_checkpointing: false  # Disabled for TPU compatibility
