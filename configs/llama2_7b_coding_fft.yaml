# configs/llama2_7b_coding_fft.yaml
# Full Fine-Tuning configuration for LLaMA2-7B on coding domain
# Matches paper's setup: 8 accelerators, batch size 128, 30k samples, 2 epochs

experiment_type: fft
domain: coding
base_model_path: null

model:
  model_name: "meta-llama/Llama-2-7b-hf"
  max_seq_length: 512  # Final attempt - TPU v4-8 OOM even at 1024
  cache_dir: "./model_cache"

training:
  output_dir: "./outputs"
  experiment_name: "llama2_7b_coding_fft_paper_replication"
  num_train_epochs: 2  # Paper uses 2 epochs
  learning_rate: 2.0e-5  # Paper's LR for LLaMA2-7B
  warmup_ratio: 0.03
  lr_scheduler_type: "linear"
  weight_decay: 0.0
  logging_steps: 10  # Log metrics for monitoring
  save_steps: 1000  # Save at end only (>938 total steps)
  eval_steps: 469  # Evaluate once per epoch (469 steps/epoch with batch=64)
  seed: 42

data:
  # Paper uses 10k from each of these 3 datasets = 30k total
  dataset_mixer:
    "tulu3_persona_python": 10000  # Tulu 3 Persona Python
    "evol_code": 10000              # Evol CodeAlpaca
    "code_alpaca": 10000            # Code-Alpaca
  max_train_samples: 30000  # Exactly 30k like paper
  max_eval_samples: 164  # HumanEval has 164 problems
  preprocessing_num_workers: 8
  cache_dir: "./data_cache"
  dataloader_num_workers: 0  # 0 for TPU compatibility
  dataloader_pin_memory: false

tpu:
  use_tpu: true
  num_tpu_cores: 8  # Use all 8 cores
  use_fsdp: true  # Enable Fully Sharded Data Parallel for memory efficiency

  # Multi-core FSDP training - Conservative settings for TPU v4-8
  # 8 cores × 2 per_core × 2 grad_accum = 32 effective batch size
  # FSDP shards model parameters, gradients, and optimizer states across cores
  batch_size_per_core: 2  # Conservative to avoid XLA compilation OOM
  gradient_accumulation_steps: 2  # Accumulate gradients for larger effective batch

  # TPU optimization
  mixed_precision: "bf16"  # bfloat16 for TPU (computation only, params in fp32)
  gradient_checkpointing: false  # DISABLED: PyTorch gradient checkpointing incompatible with TPU/XLA

  # XLA settings
  xla_use_bf16: true

# Expected training with conservative batch size:
# Effective batch per step: 2 × 8 × 2 = 32 (compromise between speed and memory)
# Steps per epoch: 30000 / 32 = 938 steps
# Total steps: 938 × 2 = 1876 steps
# Time estimate: ~6-8 sec/step × 1876 = ~3-4 hours per epoch = 6-8 hours total
# NOTE: Gradient checkpointing disabled - PyTorch's implementation incompatible with torch_xla
