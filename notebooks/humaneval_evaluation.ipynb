{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# HumanEval Evaluation for LLaMA2-7B FFT Model\n",
    "\n",
    "This notebook evaluates the Full Fine-Tuned (FFT) LLaMA2-7B model on the HumanEval benchmark.\n",
    "\n",
    "**Goal:** Match the paper's FFT baseline of **29.3% pass@1**\n",
    "\n",
    "**Model:** `Chrisfrancisque/llama2-7b-coding-fft`\n",
    "\n",
    "**Expected Runtime:** ~30-45 minutes on Colab T4 GPU\n",
    "\n",
    "---\n",
    "\n",
    "## Setup Instructions\n",
    "\n",
    "1. **Runtime → Change runtime type → GPU (T4)**\n",
    "2. Run all cells in order\n",
    "3. Results will be displayed at the bottom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install"
   },
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_deps"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install required packages\n",
    "!pip install evalplus transformers torch accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imports"
   },
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports_code"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm.auto import tqdm\n",
    "from evalplus.data import get_human_eval_plus\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "config"
   },
   "source": [
    "## 3. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "config_code"
   },
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "MODEL_NAME = \"Chrisfrancisque/llama2-7b-coding-fft\"\n",
    "\n",
    "# Evaluation configuration\n",
    "NUM_SAMPLES_PER_TASK = 1  # For pass@1\n",
    "TEMPERATURE = 0.2  # Low temperature for more deterministic outputs\n",
    "MAX_NEW_TOKENS = 512  # Maximum tokens to generate\n",
    "TOP_P = 0.95\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = Path(\"./humaneval_results\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Samples per task: {NUM_SAMPLES_PER_TASK}\")\n",
    "print(f\"Temperature: {TEMPERATURE}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "load_model"
   },
   "source": [
    "## 4. Load Model and Tokenizer\n",
    "\n",
    "This will download ~6.5 GB from HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_model_code"
   },
   "outputs": [],
   "source": [
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "\n",
    "# Set pad token if not set\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Loading model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "print(\"✓ Model loaded successfully!\")\n",
    "print(f\"Model dtype: {model.dtype}\")\n",
    "print(f\"Model device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "load_dataset"
   },
   "source": [
    "## 5. Load HumanEval Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_dataset_code"
   },
   "outputs": [],
   "source": [
    "print(\"Loading HumanEval dataset...\")\n",
    "problems = get_human_eval_plus()\n",
    "print(f\"✓ Loaded {len(problems)} problems\")\n",
    "\n",
    "# Show example problem\n",
    "example_task_id = \"HumanEval/0\"\n",
    "example_problem = problems[example_task_id]\n",
    "print(f\"\\nExample problem: {example_task_id}\")\n",
    "print(\"=\"*60)\n",
    "print(example_problem[\"prompt\"][:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "generate"
   },
   "source": [
    "## 6. Generate Code Completions\n",
    "\n",
    "This will take ~30-45 minutes for all 164 problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "generate_code"
   },
   "outputs": [],
   "source": [
    "def generate_completion(prompt, num_samples=1):\n",
    "    \"\"\"Generate code completion for a prompt\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1536)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            temperature=TEMPERATURE,\n",
    "            top_p=TOP_P,\n",
    "            num_return_sequences=num_samples,\n",
    "            do_sample=True if TEMPERATURE > 0 else False,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    completions = []\n",
    "    for output in outputs:\n",
    "        generated_tokens = output[inputs['input_ids'].shape[1]:]\n",
    "        completion = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "        completions.append(completion)\n",
    "    \n",
    "    return completions\n",
    "\n",
    "print(\"Generating completions...\")\n",
    "print(\"This will take approximately 30-45 minutes\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "samples = []\n",
    "for task_id, problem in tqdm(problems.items(), desc=\"Evaluating\"):\n",
    "    prompt = problem[\"prompt\"]\n",
    "    \n",
    "    for sample_idx in range(NUM_SAMPLES_PER_TASK):\n",
    "        completions = generate_completion(prompt, num_samples=1)\n",
    "        \n",
    "        sample = {\n",
    "            \"task_id\": task_id,\n",
    "            \"completion\": completions[0]\n",
    "        }\n",
    "        samples.append(sample)\n",
    "\n",
    "print(f\"\\n✓ Generated {len(samples)} completions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "save"
   },
   "source": [
    "## 7. Save Completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_code"
   },
   "outputs": [],
   "source": [
    "samples_file = OUTPUT_DIR / \"samples.jsonl\"\n",
    "print(f\"Saving completions to {samples_file}\")\n",
    "\n",
    "with open(samples_file, 'w') as f:\n",
    "    for sample in samples:\n",
    "        f.write(json.dumps(sample) + '\\n')\n",
    "\n",
    "print(\"✓ Completions saved\")\n",
    "\n",
    "# Show example completion\n",
    "print(\"\\nExample completion:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Task: {samples[0]['task_id']}\")\n",
    "print(f\"Completion:\\n{samples[0]['completion'][:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evaluate"
   },
   "source": [
    "## 8. Run evalplus Evaluation\n",
    "\n",
    "This executes the generated code in a sandbox and checks correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evaluate_code"
   },
   "outputs": [],
   "source": [
    "print(\"Running evalplus evaluation...\")\n",
    "print(\"This will execute generated code in a sandboxed environment\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "eval_command = [\n",
    "    \"evalplus.evaluate\",\n",
    "    \"--dataset\", \"humaneval\",\n",
    "    \"--samples\", str(samples_file),\n",
    "]\n",
    "\n",
    "result = subprocess.run(\n",
    "    eval_command,\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "print(result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"Errors:\", result.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "results"
   },
   "source": [
    "## 9. Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "results_code"
   },
   "outputs": [],
   "source": [
    "# Find the results file\n",
    "results_file = OUTPUT_DIR / \"samples_eval_results.json\"\n",
    "\n",
    "if results_file.exists():\n",
    "    with open(results_file, 'r') as f:\n",
    "        results = json.load(f)\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"FINAL RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if \"eval\" in results:\n",
    "        eval_results = results[\"eval\"]\n",
    "        \n",
    "        # Extract pass@1\n",
    "        pass_at_1 = eval_results.get(\"pass@1\", 0) * 100\n",
    "        \n",
    "        print(f\"\\n✓ Pass@1: {pass_at_1:.2f}%\")\n",
    "        print(f\"\\nPaper's FFT Baseline: 29.3%\")\n",
    "        \n",
    "        difference = pass_at_1 - 29.3\n",
    "        if difference >= 0:\n",
    "            print(f\"Difference: +{difference:.2f}% (BETTER than paper!)\")\n",
    "        else:\n",
    "            print(f\"Difference: {difference:.2f}%\")\n",
    "        \n",
    "        print(\"\\nAll metrics:\")\n",
    "        for metric, value in eval_results.items():\n",
    "            print(f\"  {metric}: {value*100:.2f}%\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Full results:\")\n",
    "    print(json.dumps(results, indent=2))\n",
    "else:\n",
    "    print(\"Results file not found. Check the evaluation output above.\")\n",
    "    print(f\"Expected file: {results_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary"
   },
   "source": [
    "## 10. Summary\n",
    "\n",
    "### Evaluation Complete!\n",
    "\n",
    "Compare your results to the paper:\n",
    "\n",
    "| Model | HumanEval Pass@1 |\n",
    "|-------|------------------|\n",
    "| LLaMA2-7B (Base) | ~15% |\n",
    "| **LLaMA2-7B FFT (Paper)** | **29.3%** |\n",
    "| **Your FFT Model** | **(See above)** |\n",
    "| LLaMA2-7B MFT (Paper) | 31.7% (+2.4%) |\n",
    "\n",
    "---\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **If pass@1 ≈ 29.3%**: Your FFT training was successful! ✓\n",
    "   - Proceed with MFT (Mask Fine-Tuning) training\n",
    "   - Goal: Achieve >31% pass@1 by removing 10% of parameters\n",
    "\n",
    "2. **If pass@1 < 25%**: Training may need adjustment\n",
    "   - Check training logs for issues\n",
    "   - Consider training longer or adjusting hyperparameters\n",
    "\n",
    "3. **Download results for your records**:\n",
    "   - Files in `./humaneval_results/`\n",
    "   - samples.jsonl (all generated code)\n",
    "   - samples_eval_results.json (evaluation results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download"
   },
   "source": [
    "## Optional: Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_code"
   },
   "outputs": [],
   "source": [
    "# Download results to your local machine\n",
    "from google.colab import files\n",
    "\n",
    "if results_file.exists():\n",
    "    print(\"Downloading results...\")\n",
    "    files.download(str(results_file))\n",
    "    print(\"✓ Download started\")\n",
    "else:\n",
    "    print(\"No results file to download\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
